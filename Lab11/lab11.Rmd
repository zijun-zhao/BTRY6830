---
title: "Quantitative Genomics and Genetics 2018"
subtitle: "Computer Lab 11"
author: "Zijun Zhao"
date: "4/26/2018"
output: html_document
---

--------------------------------------------------------------------------
### Including Covariates in Regression Models 

If genotype effects and random noise were the only variables having an effect on the phenotype (y), the models that we've used so far will be sufficient. However, in most real datasets there are many other factors that might have an influence on the phenotypes that we are interested in. For example, in gene expression measurements it has been shown that techincal artifacts, such as laboratory specific protocols or exposure to slight environmental perturbations, can cause systematic differences between samples. Since genetic effects are mostly very small, this usually results in a loss of statistical power leading to incorrect results. On top of that, a correlation structure between the independent variables (x) (for example genome wide correlated genotypes) can also obscure the output by calling too many variables significant (the problem of population structure in GWAS). Today we are going to learn how to include additional covariates in the model to account for such factors.

--------------------------------------------------------------------------

### 1. Linear Regression with Covariates

So far the regression models that we have tested were only useful for testing a null hypothesis where the genotype betas are all 0 against an alternative hypothesis where the betas for genotypes have a non-zero value. However, when including covariates we are only interested in the beta values of the genotypes not the covariate beta values. In other words, we don't really care if the covariates have an effect on the phenotype, we just want to know whether the genotypes have an effect. So now the null hypothesis has to change a little bit, and to accommodate that change we have to use a slightly different framework than before. We are going to use the likelihood ratio test for this purpose. 

The null is now :

$$y = \beta_\mu + \beta_c \cdot covariate + error$$

and the alternative is :

$$y = \beta_\mu + \beta_g \cdot genotypes + \beta_c \cdot covariate + error$$

To test the significance of genotypes in this framework, we are going to calculate the likelihood for the null and the alternative and use a likelihood ratio test similar to what we did in the logistic regression lab. In order to do this, we first have to create a function to calculate the likelihood for the model fit. 

```{r, comment = NA, echo = FALSE}

library(MASS)
library(lmtest)

lr_likelihood <- function(y, x_input = NULL){
    n_samples <- length(y)

    X_mx <- cbind(matrix(1, nrow = n_samples, ncol = 1), x_input)

    MLE_beta <- ginv(t(X_mx) %*% X_mx) %*% t(X_mx) %*% y
    
    y_hat <- X_mx %*% MLE_beta
    
    var_hat <- sum((y - (y_hat))^2) / (n_samples - 1)
    
    log_likelihood <- -((n_samples / 2) * log(2 * pi * var_hat) ) - ((1/ (2*var_hat)) * sum((y - (y_hat))^2))
    
    return(log_likelihood)
}
```

To use the likelihood ratio test, we would also need a function to calculate a p-value from two given log-likelihoods. 

```{r, comment = NA, echo = FALSE}

LRT_test <- function(logl_H0, logl_HA, df_test){

    LRT<-2*logl_HA-2*logl_H0 #likelihood ratio test statistic
    #likelihood ratio test statistic for every genotype
    pval <- pchisq(LRT, df_test, lower.tail = F)
    return(pval)
}

```

Now let's see what the effects are for covariate inclusion. 

First, simply testing the genotype effect with or without a covariate effect on the phenotype leads to different levels of significance. 

```{r, comment = NA, echo = TRUE}
set.seed(1987)

x = sample(c(-1,0,1), 100, replace = TRUE)

y = 0.9 * x + rnorm(100)

h0_nocovar <- lr_likelihood(y)
h1_nocovar <- lr_likelihood(y, x)

LRT_test(h0_nocovar, h1_nocovar, df_test = 1)
```

You can see that the introduction of additional variance that is not normal lowers the significance of the model.  

```{r, comment = NA, echo = TRUE}
x_c = sample(c(0,1), 100, replace = TRUE)
y2 = y + 0.8 * x_c 
  
h0_withcovar <- lr_likelihood(y2)
h1_withcovar <- lr_likelihood(y2, x)
LRT_test(h0_withcovar, h1_withcovar, df_test = 1)
```

This effect can be corrected by accounting for the additional variance in the null model.

```{r, comment = NA, echo = TRUE}
h0_includecovar <- lr_likelihood(y2, x_c)
ha_includecovar <- lr_likelihood(y2, cbind(x,x_c))
LRT_test(h0_includecovar, ha_includecovar, df_test = 1)
```

--------------------------------------------------------------------------

### 2. Logistic Regression with Covariates

With logistic regression, things are actually simpler since the only thing that we have to do is calculate the likelihood for H0 with covariates and modify the function slightly to incorporate additional x values. 

```{r, comment = NA, echo = TRUE, eval = FALSE}
for(j in 1:dim(xa_matrix)[2]){
  myList<-logistic.IRLS( <Run with covariates included> )
  beta<-cbind(beta,myList[[1]])
  logl<-c(logl,myList[[2]])
}

# log likelihood for NULL hypothesis
logl_H0 <- logistic.IRLS(Y=Y, <X values = 0, covariates included>)[[2]]


```


--------------------------------------------------------------------------

### 3. Exercise: Principal Components as covariates 

A common practice in the GWAS world is to include principal components (PC) in the regression model to account for variance in the expression values, or population structure in the genotypes. For the former, one would calculate the PCs using the expression values and genotypes values for the latter.  

There is no golden rule of how many of those PCs to include in the model, but for some cases people have used the percent variance explained by PCs to decide how many to include. 

Combine what we learned today and during the previous labs, produce a manhattan plot for each phenotype, including at least 1 genotype and 1 phenotype PC in your logistic regression model using the data for this lab.






